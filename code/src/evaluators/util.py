from sentence_transformers import SentenceTransformer, util
import numpy as np
from typing import List
import networkx as nx
import alibabacloud_oss_v2 as oss


def build_id_map(obj):
    """
    æ„å»ºä¸€ä¸ªä» @id åˆ°å®Œæ•´å¯¹è±¡çš„æ˜ å°„å­—å…¸
    """
    id_map = {}

    def recurse(o):
        if isinstance(o, dict):
            if "@id" in o:
                id_map[o["@id"]] = o
            for v in o.values():
                recurse(v)
        elif isinstance(o, list):
            for item in o:
                recurse(item)

    recurse(obj)
    return id_map


def resolve_refs(obj, id_map, visited=None):
    """é€’å½’æ›¿æ¢æ‰€æœ‰ value æ˜¯ id çš„å­—æ®µä¸ºå¯¹åº”å¯¹è±¡ï¼Œé¿å…æ­»å¾ªç¯ã€‚"""
    if visited is None:
        visited = set()

    if isinstance(obj, dict):
        new_obj = {}
        for k, v in obj.items():
            if isinstance(v, str) and v in id_map:
                if v in visited:
                    new_obj[k] = v  # é˜²æ­¢æ­»å¾ªç¯
                else:
                    visited.add(v)
                    new_obj[k] = resolve_refs(id_map[v], id_map, visited.copy())
            else:
                new_obj[k] = resolve_refs(v, id_map, visited.copy())
        return new_obj

    elif isinstance(obj, list):
        return [resolve_refs(item, id_map, visited.copy()) for item in obj]

    else:
        return obj


def remove_ids(o):
    """
    é€’å½’åœ°åˆ é™¤å¯¹è±¡ä¸­çš„ @id å­—æ®µï¼Œè¿”å›ä¸€ä¸ªæ–°çš„å¯¹è±¡ã€‚
    å¦‚æœå¯¹è±¡æ˜¯åˆ—è¡¨ï¼Œåˆ™è¿”å›ä¸€ä¸ªæ–°çš„åˆ—è¡¨ã€‚
    """
    if isinstance(o, dict):
        return {k: remove_ids(v) for k, v in o.items() if k != "@id"}
    elif isinstance(o, list):
        return [remove_ids(item) for item in o]
    else:
        return o
    


def dict_to_sorted_str(d):
    """
    å°†å­—å…¸è½¬æ¢ä¸ºæŒ‰ key æ’åºçš„å­—ç¬¦ä¸²ï¼Œé€‚åˆç”¨äºè¯­ä¹‰ç¼–ç 
    """
    if isinstance(d, dict):
        return "{" + ", ".join(f"{k}:{dict_to_sorted_str(v)}" for k, v in sorted(d.items())) + "}"
    elif isinstance(d, list):
        return "[" + ", ".join(dict_to_sorted_str(item) for item in d) + "]"
    else:
        return str(d)
    

def get_node_text(data: dict) -> str:
    """
    ä»èŠ‚ç‚¹æ•°æ®ä¸­æå–æ–‡æœ¬ä¿¡æ¯ï¼Œå¿½ç•¥ @id å­—æ®µ
    """

    res= ' '.join(
        f"{k}: {v}" for k, v in data.items()
        if k != '@id' and isinstance(v, (str, int, float)) and v
    )
    #print("get_node_text:", res)
    return res if res else "empty_node"


def compute_semantic_alignment(g1: nx.DiGraph, g2: nx.DiGraph, model, threshold=0.75):
    """
    è¿”å› g1 ä¸­æ¯ä¸ªèŠ‚ç‚¹ä¸ g2 ä¸­æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹çš„æ˜ å°„
    """

    g1_nodes = list(g1.nodes(data=True))
    g2_nodes = list(g2.nodes(data=True))
    #print("g1_nodes:", len(g1_nodes), "g2_nodes:", len(g2_nodes))
    # print("g1.nodes:", g1.nodes)
    # print("list of g1 nodes:", g1_nodes)
    #print("g2_nodes:", g2_nodes)
    
    g1_vecs = model.encode([get_node_text(n) for k,n in g1_nodes], convert_to_tensor=True)
    g2_vecs = model.encode([get_node_text(n) for k,n in g2_nodes], convert_to_tensor=True)

    sim_matrix = util.cos_sim(g1_vecs, g2_vecs).cpu().numpy()
    #print("similarity matrix:", sim_matrix)

    mapping = {}
    for i, (n1,n1_attr) in enumerate(g1_nodes):
        j_best = np.argmax(sim_matrix[i])
        score = sim_matrix[i][j_best]
        if score >= threshold:
            mapping[n1] = g2_nodes[j_best][0]  # ä½¿ç”¨ g2 çš„ @id ä½œä¸ºæ˜ å°„å€¼
    return mapping


import re
import json
from typing import Set, Tuple, Optional
from difflib import SequenceMatcher
from nltk.stem import WordNetLemmatizer
from code.src.database.xmi_parser.base.namespace import NS
import xml.etree.ElementTree as ET
from lxml import etree

class XMIEvaluatorUtil:
    def __init__(self, synonym_path: Optional[str] = "code/src/config/synonyms.json"):
        self.lemmatizer = WordNetLemmatizer()
        self.synonym_dict = self._load_synonym_dict(synonym_path)

    def _load_synonym_dict(self, path: Optional[str]) -> dict:
        if path:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"[WARNING] Failed to load synonym dictionary from {path}: {e}")
        # é»˜è®¤åŒä¹‰è¯æ˜ å°„
        return {
            'thermal': 'temperature',
            'structure & mechanisms': 'structure and mechanisms',
            'structure_and_mechanisms': 'structure and mechanisms',
            'gn&c': 'gnc',
            'gnc': 'guidance navigation control',
            'comm': 'communications',
            'rf': 'radio frequency'
        }

    def _normalize(self, text: str) -> str:
        if not text:
            return ""

        text = text.lower()
        text = text.replace("&", "and")
        text = re.sub(r'[^a-z0-9 ]', ' ', text)
        words = text.split()
        normalized_words = []

        for word in words:
            lemma = self.lemmatizer.lemmatize(word)
            mapped = self.synonym_dict.get(lemma, lemma)
            normalized_words.append(mapped)

        return ' '.join(normalized_words)


    def _is_match(self, items1: Tuple, items2: Tuple,similarity_threshold=0.8) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ª å…ƒç»„å„é¡¹æ˜¯å¦åŒ¹é…ã€‚
        åŸºäºé˜ˆå€¼åˆ¤æ–­æ˜¯å¦åŒ¹é…ã€‚
        """
        # xmiEvaluatorUtil = XMIEvaluatorUtil()
        for item1, item2 in zip(items1, items2):
            item_single_1 = self._normalize(item1)
            item_single_2 = self._normalize(item2)
            # print("item1:", item1, "item2:", item2)
            # print("item_single_1:", item_single_1, "item_single_2:", item_single_2)
            temp_sim = SequenceMatcher(None, item_single_1, item_single_2).ratio()
            if temp_sim < similarity_threshold:
                return False
        return True

        # type1, name1 = map(self._normalize, item1)
        # type2, name2 = map(self._normalize, item2)
        # # print("item1:", item1, "item2:", item2)
        # # print("type1:", type1, "name1:", name1)
        # # print("type2:", type2, "name2:", name2)
        # type_sim = SequenceMatcher(None, type1, type2).ratio()
        # name_sim = SequenceMatcher(None, name1, name2).ratio()

        # return type_sim >= similarity_threshold and name_sim >= similarity_threshold
    

    def build_id_map(self, xmi_str: str) -> dict:
        """
        ä» XMI å­—ç¬¦ä¸²ä¸­æ„å»ºä¸€ä¸ªä» @id åˆ°å®Œæ•´å¯¹è±¡çš„æ˜ å°„å­—å…¸
        """
        id_map = {}
        # å…ˆæ„é€ ä¸€ä¸ª XML æ ‘
        try:
            parser = ET.XMLParser(recover=True)
            root = ET.fromstring(f"<root>{xmi_str}</root>", parser=parser)
        except ET.ParseError as e:
            print(f"[ERROR] Invalid XMI content: {e}")
            return id_map
        XMI=NS['xmi']
        XMI_ID=f"{{{XMI}}}id"
        # éå† XML æ ‘ï¼Œæå– 
        for elem in root.iter():
            if XMI_ID in elem.attrib:
                id_map[elem.attrib[XMI_ID]] = elem
        return root,id_map

    def _xmi_to_graph(self, xmi_str: str) -> nx.DiGraph:
        """
        Converts XMI content to a NetworkX directed graph.
        """
        ID_LIKE_PATTERN = re.compile(r"(^|[#])_[a-zA-Z0-9_]+$")
        G = nx.DiGraph()
        try:
            parser = etree.XMLParser(recover=True)
            root = etree.fromstring(f"<root>{xmi_str}</root>", parser=parser)
            id_element_map={}

            for elem in root.iter():
                xmi_id= elem.get('{%s}id' % NS['xmi']) or elem.get('xmi:id')
                xmi_id= xmi_id.strip() if isinstance(xmi_id, str) else xmi_id
                if xmi_id:
                    id_element_map[xmi_id] = elem
                else:
                    continue



            for elem in root.iter():
                tag_name = elem.tag.split('}')[-1]
                # print("tag:", tag_name)

                xmi_id= elem.get('{%s}id' % NS['xmi']) or elem.get('xmi:id')
                xmi_id= xmi_id.strip() if isinstance(xmi_id, str) else xmi_id
                if not xmi_id:
                    continue
                node_attrs = {}
                for k, v in elem.attrib.items():
                    if k in ['xmi:id', '{%s}id' % NS['xmi']]:
                        continue
                    # elif v.strip() in id_element_map:
                    #     continue
                    elif v and ID_LIKE_PATTERN.match(v) and v.strip() in id_element_map:
                        continue
                    # elif k.endswith('id') or k.endswith('ref'):
                    #     continue
                    node_attrs[k] = v
                node_attrs['tag'] = tag_name  # æ·»åŠ æ ‡ç­¾åä½œä¸ºå±æ€§
                G.add_node(xmi_id, **node_attrs)

            # æ·»åŠ è¾¹
            for elem in root.iter():
                xmi_id = elem.get('{%s}id' % NS['xmi']) or elem.get('xmi:id')
                if not xmi_id:
                    continue

                for k, v in elem.attrib.items():
                    if k in ['xmi:id', '{%s}id' % NS['xmi']]:
                        continue

                    v= v.strip() if isinstance(v, str) else v
                    if " " in v:
                        v_list= v.split()
                        for v_item in v_list:
                            if v_item.strip() in id_element_map:
                                G.add_edge(xmi_id, v_item, label=k)
                    elif v.strip() in id_element_map:
                        G.add_edge(xmi_id, v, label=k)
                
                for child in elem:
                    child_id = child.get('{%s}id' % NS['xmi']) or child.get('xmi:id')
                    if child_id and child_id in id_element_map:
                        tag_name = child.tag.split('}')[-1]  # è·å–æ ‡ç­¾å
                        # print("tag:", tag_name)

                        G.add_edge(xmi_id, child_id, label=tag_name)
        except etree.XMLSyntaxError as e:
            print(f"[ERROR] Invalid XMI content: {e}")
        
        return G


class EarlyStopping:
    def __init__(self, patience=3, min_delta=1e-4):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.counter = 0

    def step(self, val_loss):
        if self.best_loss - val_loss > self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False  # ä¸æ—©åœ
        else:
            self.counter += 1
            return self.counter >= self.patience
        


class TrainLossCallback:
    def __init__(self, patience, model, output_path):
        self.patience = patience
        self.best_loss = float('inf')
        self.no_improve_epochs = 0
        self.model = model
        self.output_path = output_path

    def on_epoch_end(self, epoch, logs=None):
        avg_loss = logs.get('loss') if logs else None
        if avg_loss is not None:
            print(f"ğŸ“‰ Epoch {epoch+1} å¹³å‡è®­ç»ƒæŸå¤±: {avg_loss:.4f}")

            if avg_loss < self.best_loss:
                self.best_loss = avg_loss
                self.no_improve_epochs = 0
                print(f"âœ… Loss é™ä½ï¼Œä¿å­˜æ¨¡å‹ï¼ˆ{self.output_path}ï¼‰")
                self.model.save(self.output_path)
            else:
                self.no_improve_epochs += 1
                print(f"âš ï¸ Loss æœªé™ä½ï¼Œè¿ç»­ {self.no_improve_epochs} è½®")

            if self.no_improve_epochs >= self.patience:
                print(f"â›”ï¸ è¿ç»­ {self.patience} è½® loss æ— ä¸‹é™ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                return True  # è§¦å‘ early stop

        return False  # ç»§ç»­è®­ç»ƒ

from sentence_transformers import losses
from torch.utils.tensorboard import SummaryWriter

class MyLoss(losses.MultipleNegativesRankingLoss):
    def __init__(self, model, writer: SummaryWriter):
        super().__init__(model)
        self.writer = writer
        self.global_step = 0

    def forward(self, sentence_embeddings, labels=None):
        loss_value = super().forward(sentence_embeddings, labels)
        self.writer.add_scalar("Loss/train", loss_value.item(), self.global_step)
        # print(f"[TensorBoard] Step {self.global_step}, Loss: {loss_value.item():.6f}")
        self.global_step += 1
        return loss_value
    


# oss_uploader.py
import os
from typing import Optional
import alibabacloud_oss_v2 as oss
from code.src.config import config


class OSSUtil:
    def __init__(self, region=None, bucket=None, endpoint: Optional[str] = None):
        # åˆå§‹åŒ–å‡­è¯ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        credentials_provider = oss.credentials.EnvironmentVariableCredentialsProvider()

        # åŠ è½½é»˜è®¤é…ç½®
        cfg = oss.config.load_default()
        cfg.credentials_provider = credentials_provider
        cfg.region = region or config.OSS_REGION
        if endpoint:
            cfg.endpoint = endpoint

        # åˆ›å»ºå®¢æˆ·ç«¯
        self.client = oss.Client(cfg)
        self.bucket = bucket or config.OSS_BUCKET

    def upload_string(self, object_key: str, content: str):
        data = content.encode('utf-8')
        return self._put_object(object_key, data)

    def upload_file(self, object_key: str, local_file_path: str):
        if not os.path.exists(local_file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {local_file_path}")

        with open(local_file_path, 'rb') as f:
            data = f.read()

        return self._put_object(object_key, data)

    def _put_object(self, object_key: str, data: bytes):
        result = self.client.put_object(oss.PutObjectRequest(
            bucket=self.bucket,
            key=object_key,
            body=data,
        ))
        print(f"âœ… ä¸Šä¼ æˆåŠŸ: {object_key}")
        print(f"  ğŸ“¦ çŠ¶æ€ç : {result.status_code}")
        print(f"  ğŸ“„ è¯·æ±‚ ID: {result.request_id}")
        print(f"  ğŸ” ETag: {result.etag}")
        print(f"  ğŸ’¡ CRC64: {result.hash_crc64}")
        return result